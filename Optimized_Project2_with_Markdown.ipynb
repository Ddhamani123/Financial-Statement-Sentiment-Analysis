{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c832fbc0",
   "metadata": {},
   "source": [
    "# Optimized Financial Filing Analysis Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b10deba0",
   "metadata": {},
   "source": [
    "\n",
    "This notebook demonstrates an optimized approach for analyzing SEC financial filings. \n",
    "The code is modular, efficient, and includes descriptive explanations for each step.\n",
    "\n",
    "### Key Features\n",
    "- **File Downloading and Processing**: Efficient handling of SEC EDGAR data.\n",
    "- **Mapping CIKs to Tickers**: Using SEC-provided ticker files for mapping.\n",
    "- **Filing Analysis**: Counting filings by month and weekday.\n",
    "- **Sentiment Analysis**: Examining sentiment in 10-K filings.\n",
    "\n",
    "Let's dive into the analysis.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec175354",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2441af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "import re\n",
    "import requests\n",
    "from datetime import date\n",
    "import numpy as np\n",
    "\n",
    "def download_file(url, filename):\n",
    "    \"\"\"Download a file from the given URL if it doesn't already exist locally.\"\"\"\n",
    "    destfile = os.path.join(os.getcwd(), filename)\n",
    "    if not os.path.exists(destfile):\n",
    "        response = requests.get(url, stream=True)\n",
    "        with open(destfile, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "    return destfile\n",
    "\n",
    "def read_lines(filepath):\n",
    "    \"\"\"Open a file and return its contents as a list of lines.\"\"\"\n",
    "    with open(filepath, \"r\") as file:\n",
    "        return [line.strip() for line in file]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5384ed6b",
   "metadata": {},
   "source": [
    "## Processing SEC Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50269c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_data(urls, start_line=11, filing_type='10-K'):\n",
    "    \"\"\"Processes SEC master index files to extract unique CIKs for a given filing type.\"\"\"\n",
    "    cik_sets = []  # Hold sets of CIKs for each year's filings\n",
    "    for url, filename in urls:\n",
    "        filepath = download_file(url, filename)\n",
    "        lines = read_lines(filepath)[start_line:]  # Skip header rows\n",
    "        ciks = {line.split('|')[0] for line in lines if '|' in line and line.split('|')[2] == filing_type}\n",
    "        cik_sets.append(ciks)\n",
    "    return list(set.intersection(*cik_sets))\n",
    "\n",
    "# SEC master index file URLs\n",
    "data_urls = [\n",
    "    ('https://www.sec.gov/Archives/edgar/full-index/1998/QTR4/master.idx', 'master_1998_Q4.txt'),\n",
    "    ('https://www.sec.gov/Archives/edgar/full-index/1999/QTR4/master.idx', 'master_1999_Q4.txt'),\n",
    "    ('https://www.sec.gov/Archives/edgar/full-index/2000/QTR4/master.idx', 'master_2000_Q4.txt')\n",
    "]\n",
    "\n",
    "unique_ciks = process_data(data_urls)\n",
    "print(f\"The number of unique CIKs is {len(unique_ciks)}.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3e9be9d",
   "metadata": {},
   "source": [
    "## Mapping CIKs to Tickers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb507285",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def map_ciks_to_tickers(ciks, url):\n",
    "    \"\"\"Maps CIKs to their corresponding stock tickers using SEC data.\"\"\"\n",
    "    ticker_filepath = download_file(url, \"ticker.txt\")\n",
    "    with open(ticker_filepath, \"r\") as file:\n",
    "        data = file.read()\n",
    "    cik_ticker_pairs = re.findall(r'([a-z-]+)\\s+(\\d+)', data)\n",
    "    cik_to_ticker = {cik: ticker for ticker, cik in cik_ticker_pairs}\n",
    "    return {cik: cik_to_ticker.get(cik, \"\") for cik in ciks}\n",
    "\n",
    "cik_ticker_mapping = map_ciks_to_tickers(unique_ciks, 'https://www.sec.gov/include/ticker.txt')\n",
    "missing_tickers = sum(1 for ticker in cik_ticker_mapping.values() if not ticker)\n",
    "print(f\"Of the {len(cik_ticker_mapping)} CIKs, {missing_tickers} are missing a ticker.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b6e96",
   "metadata": {},
   "source": [
    "## Analyzing Filings for 1999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e69fbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_filings(urls, ciks, filing_type='8-K'):\n",
    "    \"\"\"Analyzes filings by month and day of the week for a given filing type.\"\"\"\n",
    "    filings = []\n",
    "    for url, filename in urls:\n",
    "        filepath = download_file(url, filename)\n",
    "        lines = read_lines(filepath)[11:]\n",
    "        filings.extend([(line.split('|')[0], line.split('|')[3]) for line in lines if '|' in line and line.split('|')[2] == filing_type])\n",
    "    filings = [(cik, date_string) for cik, date_string in filings if cik in ciks]\n",
    "    months = [0] * 12\n",
    "    days = [0] * 7\n",
    "    for _, date_string in filings:\n",
    "        y, m, d = map(int, date_string.split('-'))\n",
    "        date_obj = date(y, m, d)\n",
    "        months[m - 1] += 1\n",
    "        days[date_obj.weekday()] += 1\n",
    "    return months, days\n",
    "\n",
    "filing_urls = [\n",
    "    ('https://www.sec.gov/Archives/edgar/full-index/1999/QTR1/master.idx', 'master_1999_Q1.txt'),\n",
    "    ('https://www.sec.gov/Archives/edgar/full-index/1999/QTR2/master.idx', 'master_1999_Q2.txt'),\n",
    "    ('https://www.sec.gov/Archives/edgar/full-index/1999/QTR3/master.idx', 'master_1999_Q3.txt'),\n",
    "    ('https://www.sec.gov/Archives/edgar/full-index/1999/QTR4/master.idx', 'master_1999_Q4.txt')\n",
    "]\n",
    "\n",
    "months, days = analyze_filings(filing_urls, unique_ciks)\n",
    "month_labels = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "day_labels = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "print(\"Filings by Month in 1999:\")\n",
    "for month, count in zip(month_labels, months):\n",
    "    print(f\"{month}: {count}\")\n",
    "\n",
    "print(\"Filings by Day of the Week in 1999:\")\n",
    "for day, count in zip(day_labels, days):\n",
    "    print(f\"{day}: {count}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32f665f3",
   "metadata": {},
   "source": [
    "## Sentiment Analysis of 10-K Filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb79c23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def analyze_sentiment(cik, years, word_files):\n",
    "    \"\"\"\n",
    "    Performs sentiment analysis on 10-K filings for a given CIK across multiple years.\n",
    "    It uses predefined sentiment word lists for negative, positive, uncertain, and litigious terms.\n",
    "    \"\"\"\n",
    "    counts = np.zeros((len(years), len(word_files)))  # Matrix to store word counts for each sentiment\n",
    "    lengths = []  # List to store total word count for each year\n",
    "\n",
    "    for i, year in enumerate(years):\n",
    "        # Download the 10-K filing for the given year and CIK\n",
    "        url = f'https://www.sec.gov/Archives/edgar/data/{cik}/{year}.txt'\n",
    "        filepath = download_file(url, f'cik_{year}.txt')\n",
    "        with open(filepath, \"r\") as file:\n",
    "            text = file.read().replace('\\n', '|')  # Replace newlines for easier regex matching\n",
    "\n",
    "        # Extract the \"Item 1\" section (Business Description) using regex\n",
    "        section = re.findall(r'ITEM 1(.*)ITEM 2', text, flags=re.IGNORECASE)\n",
    "        words = re.findall(r'[A-Za-z]+', section[0]) if section else []\n",
    "        words = [word.upper() for word in words]  # Convert all words to uppercase\n",
    "        lengths.append(len(words))  # Total word count for this filing\n",
    "\n",
    "        # Count occurrences of words from each sentiment category\n",
    "        for j, word_list in enumerate(word_files):\n",
    "            counts[i, j] = sum(1 for word in words if word in word_list)\n",
    "\n",
    "    return counts, lengths\n",
    "\n",
    "# Load the predefined sentiment word lists (negative, positive, uncertain, litigious)\n",
    "def load_word_lists(filenames):\n",
    "    \"\"\"Loads sentiment word lists from text files.\"\"\"\n",
    "    word_lists = []\n",
    "    for filename in filenames:\n",
    "        filepath = download_file(f'https://example.com/{filename}', filename)  # Replace with actual URLs or local paths\n",
    "        with open(filepath, \"r\") as file:\n",
    "            word_lists.append([line.strip().upper() for line in file.readlines()])\n",
    "    return word_lists\n",
    "\n",
    "# File names for the sentiment word lists\n",
    "word_files = [\n",
    "    '1.9_LM_negative.txt',\n",
    "    '1.9_LM_positive.txt',\n",
    "    '1.9_LM_uncertainty.txt',\n",
    "    '1.9_LM_litigious.txt'\n",
    "]\n",
    "\n",
    "# Load word lists into memory\n",
    "word_lists = load_word_lists(word_files)\n",
    "\n",
    "# Perform sentiment analysis for a specific CIK and years\n",
    "cik = '894490'  # Replace with a valid CIK from `unique_ciks`\n",
    "years = ['1998', '1999', '2000']\n",
    "counts, lengths = analyze_sentiment(cik, years, word_lists)\n",
    "\n",
    "# Calculate percentages of words in each category\n",
    "percentages = (counts.T / lengths).T * 100  # Convert counts to percentages\n",
    "\n",
    "# Display the results\n",
    "print(\"Sentiment Analysis Results:\")\n",
    "headers = [\"Year\", \"Negative (%)\", \"Positive (%)\", \"Uncertain (%)\", \"Litigious (%)\"]\n",
    "print(f\"{headers[0]:<6} {headers[1]:<15} {headers[2]:<15} {headers[3]:<15} {headers[4]:<15}\")\n",
    "for i, year in enumerate(years):\n",
    "    print(f\"{year:<6} {percentages[i, 0]:<15.2f} {percentages[i, 1]:<15.2f} {percentages[i, 2]:<15.2f} {percentages[i, 3]:<15.2f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
