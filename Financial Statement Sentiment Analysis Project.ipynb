{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "990c957c-9c1f-46f4-b8cb-80270b82adce",
   "metadata": {},
   "source": [
    "# Financial Data Analysis with SEC EDGAR Project (Data Analytics in Finance)\n",
    "\n",
    "This project focuses on extracting and analyzing financial data for real-world firms using the SEC EDGAR system. The tasks include processing master index files, mapping company identifiers to stock tickers, analyzing corporate filings, and performing sentiment analysis on financial statements. Below are the main objectives of the project:\n",
    "\n",
    "## Objectives\n",
    "1. **Identify Firms Filing 10-Ks in Q4 (1998-2000)**  \n",
    "   Extract a list of Central Index Keys (CIKs) for firms that filed a 10-K in the fourth quarter of each year from 1998 to 2000.\n",
    "\n",
    "2. **Map CIKs to Stock Tickers**  \n",
    "   Use the SEC's ticker-to-CIK mapping to determine how many of the identified CIKs currently have an associated stock ticker.\n",
    "\n",
    "3. **Analyze 8-K Filings in 1999**  \n",
    "   - Determine the number of 8-K filings by the identified firms, categorized by month and weekday.\n",
    "   - Compute summary statistics (e.g., minimum, maximum, mean, median, standard deviation) for the number of filings per firm.\n",
    "\n",
    "4. **Sentiment Analysis of 10-K Filings**  \n",
    "   Select a firm at random and perform sentiment analysis on Item 1 (Business Description) from its 10-K filings for 1998, 1999, and 2000.\n",
    "\n",
    "---\n",
    "\n",
    "### Tools and Techniques\n",
    "- **File Handling**: Efficient downloading and processing of large data files using Python.\n",
    "- **Data Analysis**: Leveraging pandas, numpy, and Python dictionaries for data manipulation.\n",
    "- **Natural Language Processing (NLP)**: Tokenizing and analyzing text from financial filings for sentiment analysis.\n",
    "- **Statistical Summaries**: Generating descriptive statistics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6d894642-20b6-4b0f-ba18-4fec8d53adcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import requests      # For downloading files from SEC EDGAR\n",
    "from datetime import date  # For date handling\n",
    "import statistics\n",
    "import numpy as np \n",
    "import pandas as pd\n",
    "import urllib.request # Used for accessing websites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b54441-4598-4cdf-beac-c3f2140bc061",
   "metadata": {},
   "source": [
    "### Identifying Firms Filing 10-Ks in Q4 (1998-2000)\n",
    "\n",
    "This task involves determining the **Central Index Key (CIK)** for firms that filed a 10-K during the fourth quarters of 1998, 1999, and 2000. Filing dates, available in `master.idx` files, are used as the basis for this analysis.\n",
    "\n",
    "#### Task Details:\n",
    "1. Extract CIKs from the `master.idx` files for:\n",
    "   - `1998/QTR4`\n",
    "   - `1999/QTR4`\n",
    "   - `2000/QTR4`\n",
    "2. Apply the following filters to identify valid firms:\n",
    "   - Exclude any firm that filed multiple 10-Ks in a given quarter.\n",
    "   - Exclude firms that did not file a 10-K in all three years.\n",
    "3. Compile the filtered list of CIKs into a Python list named `unique_ciks`.\n",
    "4. Print the total number of unique CIKs.\n",
    "\n",
    "#### Methodology:\n",
    "- Download the `master.idx` files for the specified quarters.\n",
    "- Use these files to extract and process the filing data.\n",
    "- Ensure the filtering criteria are met to produce the final list of valid CIKs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82cc851f-2121-4419-98b7-b80c50d281f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of unique CIKs is 262.\n"
     ]
    }
   ],
   "source": [
    "# Function for downloading files from a URL and saving them locally\n",
    "def fromfile(url, filename):\n",
    "    # Download a file from a URL if it doesn't already exist locally. Returns the destination file path.\n",
    "    destfile = os.path.join(os.getcwd(), filename)\n",
    "    if not os.path.exists(destfile):  # Check if file already exists\n",
    "        urllib.request.urlretrieve(url, destfile)\n",
    "    return destfile\n",
    "\n",
    "# Function to read a file and return its contents as a list of lines\n",
    "def read_lines(filepath):\n",
    "    # Read a file and return its contents as a list of stripped lines.\n",
    "    with open(filepath, 'r') as file:  # Use context manager for file handling\n",
    "        return [line.strip() for line in file.readlines()]\n",
    "\n",
    "# Create a user-agent for accessing websites\n",
    "opener = urllib.request.build_opener()\n",
    "opener.addheaders = [('User-agent', 'Mozilla/5.0 (Dhairya Dhamani dhamani@bc.edu)')]\n",
    "urllib.request.install_opener(opener)\n",
    "\n",
    "# File URLs for Q4 data\n",
    "urls = {\n",
    "    \"1998\": \"https://www.sec.gov/Archives/edgar/full-index/1998/QTR4/master.idx\",\n",
    "    \"1999\": \"https://www.sec.gov/Archives/edgar/full-index/1999/QTR4/master.idx\",\n",
    "    \"2000\": \"https://www.sec.gov/Archives/edgar/full-index/2000/QTR4/master.idx\"\n",
    "}\n",
    "\n",
    "# Download and read files into lists\n",
    "lines_by_year = {}\n",
    "for year, url in urls.items():\n",
    "    filename = f\"master_{year}_Q4.txt\"\n",
    "    destfile = fromfile(url, filename)\n",
    "    lines = read_lines(destfile)\n",
    "    lines_by_year[year] = lines[11:]  # Remove the first 10 rows (irrelevant data)\n",
    "\n",
    "# Extract and filter CIKs for 10-K filings\n",
    "def extract_ciks(lines):\n",
    "    # Extract unique CIKs from the provided lines, filtering for '10-K' filing type.\n",
    "    cik_data = [line.split('|') for line in lines]\n",
    "    ciks = [data[0] for data in cik_data if len(data) > 2 and data[2] == '10-K']\n",
    "    return list(set(ciks))  # Use set to remove duplicates\n",
    "\n",
    "ciks_by_year = {year: extract_ciks(lines) for year, lines in lines_by_year.items()}\n",
    "\n",
    "# Identify CIKs present in all three years\n",
    "unique_ciks = list(\n",
    "    set(ciks_by_year[\"1998\"]).intersection(ciks_by_year[\"1999\"], ciks_by_year[\"2000\"])\n",
    ")\n",
    "\n",
    "print(f\"The number of unique CIKs is {len(unique_ciks)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "510d1e5a-0f1b-41e2-8e4e-49191dcb9741",
   "metadata": {},
   "source": [
    "### Mapping CIKs to Stock Tickers\n",
    "\n",
    "This task involves mapping the Central Index Keys (CIKs) identified earlier to their corresponding stock tickers using the SEC's **ticker-to-CIK mapping file** available at [https://www.sec.gov/include/ticker.txt](https://www.sec.gov/include/ticker.txt).\n",
    "\n",
    "#### Task Details:\n",
    "1. **Data Source**:\n",
    "   - Download the `ticker.txt` file from the SEC website.\n",
    "   - This file contains mappings between stock tickers (in lowercase) and CIKs.\n",
    "\n",
    "2. **Processing**:\n",
    "   - Use a regular expression to parse the `ticker.txt` file into a list of tuples, where each tuple contains:\n",
    "     - Ticker (converted to uppercase letters).\n",
    "     - Corresponding CIK.\n",
    "   - Create a dictionary named `cik_ticker` that maps each CIK in the `unique_ciks` list to its ticker.\n",
    "\n",
    "3. **Handling Missing Data**:\n",
    "   - If a CIK from `unique_ciks` is not present in `ticker.txt`, its value in the dictionary should be set to an empty string (`\"\"`).\n",
    "   - If a CIK in `ticker.txt` does not appear in `unique_ciks`, it should be excluded from the dictionary.\n",
    "\n",
    "4. **Output**:\n",
    "   - Print the total number of CIKs in the dictionary.\n",
    "   - Print the number of CIKs that do not have an associated ticker (i.e., with a value of `\"\"`).\n",
    "\n",
    "#### Methodology:\n",
    "- Use a regular expression to efficiently parse the mapping file.\n",
    "- Construct the dictionary by iterating over the `unique_ciks` list and checking for matches in the parsed data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7f50f94b-e4e8-4072-b6d5-201f7ed29885",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Of the 262 CIKs in the cik_ticker dictionary, 211 are missing a ticker.\n"
     ]
    }
   ],
   "source": [
    "# Download the ticker data from SEC website\n",
    "destfile4 = fromfile('https://www.sec.gov/include/ticker.txt', 'ticker.txt')\n",
    "\n",
    "# Read the ticker data as a string\n",
    "with open(destfile4, 'r') as file:\n",
    "    lines4 = file.read()\n",
    "\n",
    "# Use regex to parse ticker and CIK data into a list of tuples\n",
    "ff = re.findall(r'([a-z-?]+)\\s+(\\d+)', lines4)\n",
    "\n",
    "# Create a dictionary to map CIKs to tickers\n",
    "cik_ticker = {}\n",
    "\n",
    "# Extract tickers and CIKs from the parsed data\n",
    "ticker_list = [element[0].upper() for element in ff]  # Convert tickers to uppercase\n",
    "cik_list = [element[1] for element in ff]\n",
    "\n",
    "# Iterate through unique_ciks to build the cik_ticker dictionary\n",
    "for cik in unique_ciks:\n",
    "    # If CIK is not in the parsed CIK list, assign an empty string as the value\n",
    "    if cik not in cik_list:\n",
    "        cik_ticker[cik] = \"\"\n",
    "    else:\n",
    "        # Find the corresponding ticker for the CIK\n",
    "        index = cik_list.index(cik)\n",
    "        cik_ticker[cik] = ticker_list[index]\n",
    "\n",
    "# Calculate total CIKs and missing tickers\n",
    "total = len(cik_ticker)\n",
    "missing = sum(1 for ticker in cik_ticker.values() if ticker == \"\")\n",
    "\n",
    "# Print the results\n",
    "print(f'Of the {total} CIKs in the cik_ticker dictionary, {missing} are missing a ticker.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28437f81-74ff-465f-9d9e-2f4bd71d46e3",
   "metadata": {},
   "source": [
    "### Analyzing 8-K Filings by Month and Day of the Week in 1999\n",
    "\n",
    "This task involves analyzing the filing dates of **Form 8-K** for companies in the `unique_ciks` set using the `master.idx` files for all quarters of 1999. The analysis focuses on counting the total number of 8-K filings during:\n",
    "\n",
    "- **Each month of 1999** (JAN, FEB, ..., DEC)\n",
    "- **Each day of the week during 1999** (MON, TUE, ..., SUN)\n",
    "\n",
    "#### Task Details:\n",
    "1. **Data Source**:\n",
    "   - Use the `master.idx` files for the following quarters:\n",
    "     - `1999/QTR1`\n",
    "     - `1999/QTR2`\n",
    "     - `1999/QTR3`\n",
    "     - `1999/QTR4`\n",
    "   - Each file provides filing dates and associated CIKs.\n",
    "\n",
    "2. **Filtering Criteria**:\n",
    "   - Include only **8-K filings**.\n",
    "   - Consider only the CIKs from the `unique_ciks` list.\n",
    "\n",
    "3. **Output**:\n",
    "   - Total number of 8-K filings for each month of 1999.\n",
    "   - Total number of 8-K filings for each day of the week in 1999.\n",
    "\n",
    "4. **Additional Information**:\n",
    "   - Learn more about Form 8-K [here](https://www.sec.gov/answers/form8k.htm).\n",
    "\n",
    "#### Methodology:\n",
    "- Extract filing dates for 8-K filings from the `master.idx` files.\n",
    "- Categorize filings by month and weekday using Python's date-handling capabilities.\n",
    "- Aggregate the counts and display the results for each category.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "60517ef1-df9f-4ec4-84d4-5d79105d5865",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month in 1999\n",
      "-------------\n",
      "JAN: 30\n",
      "FEB: 22\n",
      "MAR: 24\n",
      "APR: 47\n",
      "MAY: 47\n",
      "JUN: 30\n",
      "JUL: 55\n",
      "AUG: 22\n",
      "SEP: 41\n",
      "OCT: 35\n",
      "NOV: 33\n",
      "DEC: 25\n",
      "\n",
      "Day in 1999\n",
      "-----------\n",
      "MON: 59\n",
      "TUE: 72\n",
      "WED: 85\n",
      "THU: 86\n",
      "FRI: 109\n",
      "SAT: 0\n",
      "SUN: 0\n"
     ]
    }
   ],
   "source": [
    "# Download master index files for the first three quarters of 1999\n",
    "qtr_urls = [\n",
    "    ('https://www.sec.gov/Archives/edgar/full-index/1999/QTR1/master.idx', 'master_1999_Q1.txt'),\n",
    "    ('https://www.sec.gov/Archives/edgar/full-index/1999/QTR2/master.idx', 'master_1999_Q2.txt'),\n",
    "    ('https://www.sec.gov/Archives/edgar/full-index/1999/QTR3/master.idx', 'master_1999_Q3.txt'),\n",
    "]\n",
    "\n",
    "# Download files and read their contents\n",
    "lines_by_quarter = []\n",
    "for url, filename in qtr_urls:\n",
    "    destfile = fromfile(url, filename)\n",
    "    lines = read_lines(destfile)\n",
    "    lines_by_quarter.append(lines[11:])  # Strip irrelevant header rows\n",
    "\n",
    "# Add Q4 data (already downloaded)\n",
    "lines_by_quarter.append(read_lines(destfile2)[11:])\n",
    "\n",
    "# Combine all quarter data into one list\n",
    "lines2a = sum(lines_by_quarter, [])\n",
    "\n",
    "# Parse data to extract CIK, filing type, and filing date\n",
    "parsed_data = [\n",
    "    (line.split('|')[0], line.split('|')[2], line.split('|')[3])\n",
    "    for line in lines2a\n",
    "    if '|' in line\n",
    "]\n",
    "\n",
    "# Filter for 8-K filings and keep only CIK and filing date\n",
    "filtered_data = [(entry[0], entry[2]) for entry in parsed_data if entry[1] == '8-K']\n",
    "\n",
    "# Initialize counters for month and day of the week\n",
    "month_counts = [0] * 12\n",
    "weekday_counts = [0] * 7\n",
    "\n",
    "# Process each filing\n",
    "for cik, filing_date in filtered_data:\n",
    "    # Parse the filing date\n",
    "    y, m, d = map(int, filing_date.split('-'))\n",
    "    filing_day = date(y, m, d).weekday()  # Get the weekday (0=Mon, ..., 6=Sun)\n",
    "\n",
    "    # Check if the CIK is in the list of unique CIKs\n",
    "    if cik in unique_ciks:\n",
    "        month_counts[m - 1] += 1  # Increment the counter for the filing's month\n",
    "        weekday_counts[filing_day] += 1  # Increment the counter for the filing's day\n",
    "\n",
    "# Print results\n",
    "months_in_year = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "days_in_week = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "\n",
    "print('Month in 1999')\n",
    "print('-------------')\n",
    "for month, count in zip(months_in_year, month_counts):\n",
    "    print(f'{month.upper()}: {count}')\n",
    "\n",
    "print()\n",
    "print('Day in 1999')\n",
    "print('-----------')\n",
    "for day, count in zip(days_in_week, weekday_counts):\n",
    "    print(f'{day.upper()}: {count}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f053557b-bc78-4433-a4ad-6740087b4c06",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculating 8-K Filing Statistics for CIKs in 1999\n",
    "\n",
    "In this task, we calculate the total number of **Form 8-K** filings made by each CIK in the `unique_ciks` list during 1999. The objective is to compute key statistics summarizing the filing activity.\n",
    "\n",
    "#### Task Details:\n",
    "1. **Data Source**:\n",
    "   - Use the 8-K filing data from the `master.idx` files for all quarters of 1999:\n",
    "     - `1999/QTR1`\n",
    "     - `1999/QTR2`\n",
    "     - `1999/QTR3`\n",
    "     - `1999/QTR4`\n",
    "   - Ensure that only filings by the `unique_ciks` list are considered.\n",
    "\n",
    "2. **Output**:\n",
    "   - Create a dictionary or data structure where each CIK maps to the total number of 8-K filings made during 1999. If a CIK has no filings, its total should be `0`.\n",
    "   - Calculate and print the following statistics based on the totals:\n",
    "     - Minimum\n",
    "     - Maximum\n",
    "     - Mean\n",
    "     - Median\n",
    "     - Standard deviation\n",
    "\n",
    "#### Methodology:\n",
    "- Extract filing data for 8-Ks and aggregate the counts by CIK.\n",
    "- Use Python libraries such as `numpy` or `pandas` for statistical calculations.\n",
    "- Handle cases where a CIK has no filings explicitly by initializing totals to `0`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2874dbd8-92d0-49e7-9892-338509a17a8c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary statistics of 8-Ks filed by CIKs during 1999\n",
      "Minimum:  0.00\n",
      "Maximum: 13.00\n",
      "Mean:     1.57\n",
      "Median:   1.00\n",
      "St Dev:   2.30\n"
     ]
    }
   ],
   "source": [
    "# Create a dictionary to track the total number of 8-Ks filed by each CIK in 1999\n",
    "tot_8K = {cik: 0 for cik in unique_ciks}  # Initialize all counts to 0\n",
    "\n",
    "# Count 8-K filings for each CIK in the list\n",
    "for cik, filing_date in filtered_data:\n",
    "    if cik in tot_8K:\n",
    "        tot_8K[cik] += 1  # Increment the count for the matching CIK\n",
    "\n",
    "# Extract the counts into a list for statistical analysis\n",
    "tot_8K_values = list(tot_8K.values())\n",
    "\n",
    "# Calculate summary statistics using built-in functions\n",
    "print(\"Summary statistics of 8-Ks filed by CIKs during 1999\")\n",
    "print(f\"Minimum: {min(tot_8K_values):5.2f}\")\n",
    "print(f\"Maximum: {max(tot_8K_values):5.2f}\")\n",
    "print(f\"Mean:    {statistics.mean(tot_8K_values):5.2f}\")\n",
    "print(f\"Median:  {statistics.median(tot_8K_values):5.2f}\")\n",
    "print(f\"St Dev:  {statistics.stdev(tot_8K_values):5.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef64596-3146-4919-a7af-b7c18b30cc27",
   "metadata": {},
   "source": [
    "### Extracting and Analyzing Business Section of 10-Ks (1998-2000)\n",
    "\n",
    "In this task, we will analyze the **business section (Section 1)** of the 10-K filings for a randomly selected firm from the `unique_ciks` list. The objective is to extract the relevant section from the filings for 1998, 1999, and 2000 and perform sentiment analysis using predefined word lists.\n",
    "\n",
    "#### Task Details:\n",
    "1. **Selecting a Firm**:\n",
    "   - Pick one CIK randomly from the `unique_ciks` list.\n",
    "\n",
    "2. **Downloading 10-Ks**:\n",
    "   - Locate the file paths for the 10-K filings in the `master.idx` files for `1998/QTR4`, `1999/QTR4`, and `2000/QTR4`.\n",
    "   - Download the corresponding filings for the selected CIK.\n",
    "\n",
    "3. **Extracting Section 1**:\n",
    "   - Use a **regular expression** to extract Section 1 (the business description) as a long string.\n",
    "   - If necessary, use different regular expressions for each year due to potential differences in formatting.\n",
    "   - Replace newline characters (`\\n`) with a delimiter (e.g., `|`) to simplify text processing.\n",
    "\n",
    "4. **Sentiment Analysis**:\n",
    "   - Convert the extracted text to a list of uppercase words.\n",
    "   - Compare the words to predefined word lists for the following sentiment categories:\n",
    "     - **Negative**\n",
    "     - **Positive**\n",
    "     - **Uncertain**\n",
    "     - **Litigious**\n",
    "\n",
    "5. **Constructing the Table**:\n",
    "   - Create a table with:\n",
    "     - **Rows**: One for each year (1998, 1999, 2000).\n",
    "     - **Columns**: Fraction of words classified as negative, positive, uncertain, and litigious.\n",
    "   - Format percentages to two decimal places (e.g., 1.00%).\n",
    "\n",
    "#### Output:\n",
    "- A formatted table showing the fraction of words in Section 1 of each 10-K classified under the four sentiment categories.\n",
    "\n",
    "#### Additional Notes:\n",
    "- If the 10-K is in HTML format, you may need to clean the HTML tags using regex or select a different firm.\n",
    "- Ensure all text processing is case-insensitive by converting text to uppercase before analysis.\n",
    "- Learn more about the structure of a 10-K [here](https://www.sec.gov/answers/reada10k.htm).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "69028547-9ac7-4cb1-a795-3fbc230aa3af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1998 Total Words: 15387\n",
      "1999 Total Words: 14937\n",
      "2000 Total Words: 14110\n",
      "\n",
      "Year   Neg. (%)   Pos. (%)   Unc. (%)   Lit. (%)  \n",
      "--------------------------------------------------\n",
      "1998     1.42%      0.55%      1.15%      0.94%\n",
      "1999     1.53%      0.59%      1.01%      0.90%\n",
      "2000     1.55%      0.55%      1.06%      1.05%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "# Load word lists\n",
    "def load_word_list(filename):\n",
    "    # Load a word list from a file, stripping whitespace and converting to uppercase.\n",
    "    with open(filename, 'r') as f:\n",
    "        return [line.strip().upper() for line in f.readlines()]\n",
    "\n",
    "list_neg = load_word_list('1.9_LM_negative.txt')\n",
    "list_pos = load_word_list('1.9_LM_positive.txt')\n",
    "list_unc = load_word_list('1.9_LM_uncertainty.txt')\n",
    "list_lit = load_word_list('1.9_LM_litigious.txt')\n",
    "\n",
    "# Set the CIK to analyze\n",
    "my_cik = '894490'  # Replace with your desired CIK\n",
    "\n",
    "# Download the 10-K filings for 1998, 1999, and 2000\n",
    "urls = {\n",
    "    \"1998\": f'https://www.sec.gov/Archives/edgar/data/{my_cik}/0000950170-98-002420.txt',\n",
    "    \"1999\": f'https://www.sec.gov/Archives/edgar/data/{my_cik}/0000950170-99-001955.txt',\n",
    "    \"2000\": f'https://www.sec.gov/Archives/edgar/data/{my_cik}/0000950170-00-002076.txt',\n",
    "}\n",
    "\n",
    "files = {}\n",
    "for year, url in urls.items():\n",
    "    filename = f'cik_{year}.txt'\n",
    "    if not os.path.exists(filename):\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "    files[year] = filename\n",
    "\n",
    "# Read and clean the contents of each 10-K\n",
    "def read_file(filepath):\n",
    "    # Read the file and replace newline characters with '|'.\n",
    "    with open(filepath, 'r') as f:\n",
    "        return f.read().replace('\\n', '|')\n",
    "\n",
    "lines = {year: read_file(filepath) for year, filepath in files.items()}\n",
    "\n",
    "# Extract Section 1 (Item 1) with original regex logic\n",
    "def process_section(lines, year):\n",
    "    # Extract and clean words from Section 1 (Item 1) using the original regex logic.\n",
    "    if year == \"2000\":\n",
    "        # Use the 2000-specific regex\n",
    "        section = re.findall('Item 1.(.*)Item 2.', lines)\n",
    "    else:\n",
    "        # Use the 1998/1999 regex\n",
    "        section = re.findall('ITEM 1(.*)ITEM 2.', lines)\n",
    "\n",
    "    # Extract words from the matched section\n",
    "    if section:\n",
    "        words = re.findall('[A-Za-z]+', section[0])  # Only alphabetic words\n",
    "        words = [word.upper() for word in words if word != '']  # Convert to uppercase and remove empty words\n",
    "\n",
    "        # Remove unwanted tokens ('S', 'C')\n",
    "        words = [word for word in words if word not in {'S', 'C'}]\n",
    "        return words\n",
    "    return []\n",
    "\n",
    "# Process Section 1 for each year\n",
    "sect1_1998 = process_section(lines[\"1998\"], \"1998\")\n",
    "sect1_1999 = process_section(lines[\"1999\"], \"1999\")\n",
    "sect1_2000 = process_section(lines[\"2000\"], \"2000\")\n",
    "\n",
    "# Debug: Print total words extracted for each year\n",
    "print(f\"1998 Total Words: {len(sect1_1998)}\")\n",
    "print(f\"1999 Total Words: {len(sect1_1999)}\")\n",
    "print(f\"2000 Total Words: {len(sect1_2000)}\")\n",
    "print()\n",
    "\n",
    "# Perform sentiment analysis\n",
    "word_lists = [list_neg, list_pos, list_unc, list_lit]\n",
    "sections = [sect1_1998, sect1_1999, sect1_2000]\n",
    "results = np.zeros((3, 4))  # 3 rows (years) x 4 columns (sentiments)\n",
    "lengths = [len(section) for section in sections]\n",
    "\n",
    "for row, section in enumerate(sections):\n",
    "    for col, word_list in enumerate(word_lists):\n",
    "        results[row, col] = sum(1 for word in section if word in word_list)\n",
    "\n",
    "# Calculate fractions and format results\n",
    "fractions = np.round((results.T / lengths).T * 100, 2)  # Convert counts to percentages\n",
    "years = ['1998', '1999', '2000']\n",
    "columns = ['Neg.', 'Pos.', 'Unc.', 'Lit.']\n",
    "\n",
    "# Print the results in a properly aligned table\n",
    "header = f'{\"Year\":<6} {\"Neg. (%)\":<10} {\"Pos. (%)\":<10} {\"Unc. (%)\":<10} {\"Lit. (%)\":<10}'\n",
    "print(header)\n",
    "print('-' * len(header))  # Add a separator line for clarity\n",
    "\n",
    "for i, year in enumerate(years):\n",
    "    print(f'{year:<6} {fractions[i, 0]:>6.2f}% {fractions[i, 1]:>9.2f}% {fractions[i, 2]:>9.2f}% {fractions[i, 3]:>9.2f}%')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
